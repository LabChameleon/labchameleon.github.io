@article{BecEtAl2024,
  abbr={EWRL},
  title={ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning},
  author={Becktepe*, Jannis and Dierkes*, Julian and Benjamins, Carolin and Mohan, Aditya and Selinas, David and Rajan, Raghu and Hutter, Frank and Hoos, Holger and Lindauer, Marius and Eimer, Theresa},
  abstract={Hyperparameters are a critical factor in reliably training well-performing reinforcement learning (RL) agents. Unfortunately, developing and evaluating automated approaches for tuning such hyperparameters is both costly and time-consuming. As a result, such approaches are often only evaluated on a single domain or algorithm, making comparisons difficult and limiting insights into their generalizability. We propose ARLBench, a benchmark for hyperparameter optimization (HPO) in RL that allows comparisons of diverse HPO approaches while being highly efficient in evaluation. To enable research into HPO in RL, even in settings with low compute resources, we select a representative subset of HPO tasks spanning a variety of algorithm and environment combinations. This selection allows for generating a performance profile of an automated RL (AutoRL) method using only a fraction of the compute previously necessary, enabling a broader range of researchers to work on HPO in RL. With the extensive and large-scale dataset on hyperparameter landscapes that our selection is based on, ARLBench is an efficient, flexible, and future-oriented foundation for research on AutoRL.},
  journal={17th European Workshop on Reinforcement Learning},
  pages={1--10},
  year={2024},
  month={October},
  url={https://arxiv.org/pdf/2409.18827},
  pdf={https://arxiv.org/pdf/2409.18827},
  additional_info={. *GitHub Repo* can be [found here](https://github.com/automl/arlbench)},
  selected={true},
}

@article{DieEtAl2024,
  abbr={RLC},
  title={Combining Automated Optimisation of Hyperparameters and Reward Shape},
  author={Dierkes, Julian and Cramer, Emma and Hoos, Holger and Trimpe, Sebastian},
  abstract={There has been significant progress in deep reinforcement learning (RL) in recent years. Nevertheless, finding suitable hyperparameter configurations and reward functions remains challenging even for experts, and performance heavily relies on these design choices. Also, most RL research is conducted on known benchmarks where knowledge about these choices already exists. However, novel practical applications often pose complex tasks for which no prior knowledge about good hyperparameters and reward functions is available, thus necessitating their derivation from scratch. Prior work has examined automatically tuning either hyperparameters or reward functions individually. We demonstrate empirically that an RL algorithm's hyperparameter configurations and reward function are often mutually dependent, meaning neither can be fully optimised without appropriate values for the other. We then propose a methodology for the combined optimisation of hyperparameters and the reward function. Furthermore, we include a variance penalty as an optimisation objective to improve the stability of learned policies. We conducted extensive experiments using Proximal Policy Optimisation and Soft Actor-Critic on four environments. Our results show that combined optimisation significantly improves over baseline performance in half of the environments and achieves competitive performance in the others, with only a minor increase in computational costs. This suggests that combined optimisation should be best practice.},
  journal={Proc. of the Reinforcement Learning Journal},
  volume={1},
  pages={1--9},
  year={2024},
  month={August},
  url={https://rlj.cs.umass.edu/2024/papers/Paper188.html},
  pdf={https://arxiv.org/pdf/2406.18293},
  additional_info={. *GitHub Repo* can be [found here](https://github.com/ADA-research/combined_hpo_and_reward_shaping)},
  selected={true},
}

@article{VieEtAl2023,
  abbr={ICASSPW},
  title={Efficient Utilization of Large Pre-Trained Models for Low Resource ASR},
  author={Vieting, Peter and L{\"u}scher, Christoph and Dierkes, Julian and Schl{\"u}ter, Ralf and
	  Ney, Hermann},
  abstract={Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pre-training techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.},
  journal={IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops},
  volume={1},
  pages={1--5},
  year={2023},
  month={June},
  url={https://ieeexplore.ieee.org/abstract/document/10193570},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193570},
  selected={true},
}
